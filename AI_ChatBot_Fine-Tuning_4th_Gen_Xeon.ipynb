{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e1cf93-a931-4170-bcda-c5c4e50c0aa4",
   "metadata": {},
   "source": [
    "# Transfer Learning a Bert Model for an Airline Chatbot with Intel Extension for Pytorch on 4th Gen Xeon\n",
    "\n",
    "In this article, we will explore an AI system that understands the intent and the entities involved within the query, lookup or launch the relevant information, and return the appropriate response to the customer in a reasonable amount of time. We leverage the Intel Extension for PyTorch to fine-tune a foundational BertModel from the Hugging Face Transformers library to train and deploy an accurate and quick AI system to predict the Intent and Entities of a user requesting information about airline travel.\n",
    "\n",
    "<img src=\"Assets/chatbot.jpg\" style=\"border-radius:10px\" width=\"750\">\n",
    "\n",
    "If you'd like to replicate this work with the same development environment, please review the instructions in the repository's readme which explain how to get access to the Intel Developer Cloud's 4th Generation Xeon Instances. \n",
    "\n",
    "#### Contents: \n",
    "1. Setting Up Your Environment \n",
    "2. Our Dataset: Airline Travel Information Systems (ATIS)\n",
    "3. Building an AI-Powered Customer Care Chatbot\n",
    "   - Import Libraries\n",
    "   - Data Processing and Data Loader\n",
    "   - Model Defintion\n",
    "   - Evaluation Function\n",
    "   - Model Training Function\n",
    "   - Setting File Paths and Hyperparameters\n",
    "   - Tokenizing Dataset (Airline Travel Info Systems - ATIS)\n",
    "   - Model Training and Evaluation IPEX AMX and AMP (bf16 enabled)\n",
    "   - Model Training and Evaluation of IPEX FP32 with AVX-512\n",
    "4. Calculating Accuracy Scores, Training, and Inference Times\n",
    "5. Conclusion and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af830ad-a3ec-4135-8a40-1b7b3b23ceef",
   "metadata": {},
   "source": [
    "# 1. Setting up your Environment\n",
    "\n",
    "Feel free to use the conda environment .yml configuration below to set up your environment.\n",
    "\n",
    "```\n",
    "name: chatbot\n",
    "channels:\n",
    "  - pytorch\n",
    "  - intel\n",
    "dependencies:\n",
    "  - intel::intelpython3_core\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - pytorch::pytorch==1.11.0\n",
    "  - cpuonly\n",
    "  - conda-forge::scikit-learn\n",
    "  - neural-compressor\n",
    "  - pip:\n",
    "    - intel_extension_for_pytorch==1.11.200\n",
    "    - psutil\n",
    "    - transformers\n",
    "    - torchserve\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fece399-17dd-498c-9def-a709352cdb6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Our Dataset: Airline Travel Information Systems (ATIS)\n",
    "\n",
    "<img src=\"Assets/wing-airplane-flying-sea-island.jpg\" style=\"border-radius:10px\" width=\"750\">\n",
    "\n",
    "This demo will use the Airline Travel Information Systems (ATIS) dataset. The dataset consists of ~5000 queries of customer requests for flight-related details. Each of these queries is annotated with the intent and the entities involved within the query. For example, the phrase\n",
    "\n",
    "I want to fly from Orlando to Houston round trip.\n",
    "\n",
    "Would be classified with the intent of atis_flight, corresponding to a flight reservation, and the entities would be `Orlando (fromloc.city_name)`, `Houston(toloc.city_name)`, and `round_trip (round_trip)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51021526-4320-42bb-ac5d-8ab03b2c06ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Building an AI-Powered Customer Care Chatbot\n",
    "## Transfer Learning with Intel Extension for PyTorch, AMX, and AMP\n",
    "\n",
    "Briefly, given a customer query, the AI system must understand the intent and the entities involved within the query, lookup or launch the relevant information, and return the appropriate response to the customer in a reasonable amount of time. In this example, we focus on leveraging the Intel® oneAPI AI Analytics Toolkit on the task of training and deploying an accurate and quick AI system to predict the Intent and Entities of a user query.\n",
    "\n",
    "First, check if your machine has AMX enabled by running `!lscpu | grep amx` in your terminal.\n",
    "\n",
    "If AMX is not available, you will have to set `amx= False` in the training function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82dac0-5db8-426c-90d2-3c855df20ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92bee0-50aa-45b2-977f-5759c3e85c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gives control over the number of threads \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "\n",
    "# manages the amount of time before cores go back to sleep between processes\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"50\"\n",
    "\n",
    "# use top + 1 + t , to check the utilization of your cores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65d36d-d0de-4ed4-87db-701998fd6f34",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd84957-be2d-4b07-bade-1279292e201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import operator\n",
    "\n",
    "from typing import Any, Dict, List, Union\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load intel extension for pytorch\n",
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44b090-1bc0-47d3-8542-9905d13a01bb",
   "metadata": {},
   "source": [
    "### 3.2 Data Processing and Data Loader\n",
    "\n",
    "A few utility functions to read and process the ATIS dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169ddc8-c291-4936-8a8f-3a3547bce35a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code adopted from\n",
    "https://github.com/sz128/slot_filling_and_intent_detection_of_SLU\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def construct_vocab(\n",
    "    input_seqs: List[str],\n",
    "    vocab_config: Dict[str, Any] = None\n",
    ") -> Union[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Construct a vocabulary given a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        input_seqs (List[str]): list of sentences\n",
    "        vocab_config (Dict[str, Any], optional): options for constructing\n",
    "            the vocab. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Union[Dict[str,int], Dict[int, str]]: dictionarys for lookup and\n",
    "            reverse lookup\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_config is None:\n",
    "        vocab_config = {'mini_word_freq': 1, 'bos_eos': False}\n",
    "\n",
    "    vocab = {}\n",
    "    for seq in input_seqs:\n",
    "        if isinstance(seq, type([])):\n",
    "            for word in seq:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] += 1\n",
    "        else:\n",
    "            if seq not in vocab:\n",
    "                vocab[seq] = 1\n",
    "            else:\n",
    "                vocab[seq] += 1\n",
    "\n",
    "    # Discard start, end, pad and unk tokens if already present\n",
    "    if '<s>' in vocab:\n",
    "        del vocab['<s>']\n",
    "    if '<pad>' in vocab:\n",
    "        del vocab['<pad>']\n",
    "    if '</s>' in vocab:\n",
    "        del vocab['</s>']\n",
    "    if '<unk>' in vocab:\n",
    "        del vocab['<unk>']\n",
    "\n",
    "    if vocab_config['bos_eos'] is True:\n",
    "        word2id = {'<pad>': 0, '<unk>': 1, '<s>': 2, '</s>': 3}\n",
    "        id2word = {0: '<pad>', 1: '<unk>', 2: '<s>', 3: '</s>'}\n",
    "    else:\n",
    "        word2id = {'<pad>': 0, '<unk>': 1, }\n",
    "        id2word = {0: '<pad>', 1: '<unk>', }\n",
    "\n",
    "    sorted_word2id = sorted(\n",
    "        vocab.items(),\n",
    "        key=operator.itemgetter(1),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    sorted_words = [x[0] for x in sorted_word2id if x[1]\n",
    "                    >= vocab_config['mini_word_freq']]\n",
    "\n",
    "    for word in sorted_words:\n",
    "        idx = len(word2id)\n",
    "        word2id[word] = idx\n",
    "        id2word[idx] = word\n",
    "\n",
    "    return word2id, id2word\n",
    "\n",
    "\n",
    "def read_vocab_file(\n",
    "        vocab_path: str,\n",
    "        bos_eos: bool = False,\n",
    "        no_pad: bool = False,\n",
    "        no_unk: bool = False,\n",
    "        separator: str = ':'\n",
    ") -> Union[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Reads a pre-existing vocabulary.\n",
    "\n",
    "    Args:\n",
    "        vocab_path (str): path to vocab file\n",
    "        bos_eos (bool, optional): add begining and ending. Defaults to False.\n",
    "        no_pad (bool, optional): use pad tokens. Defaults to False.\n",
    "        no_unk (bool, optional): use unknown tokens. Defaults to False.\n",
    "        separator (str, optional): separator token  to use. Defaults to ':'.\n",
    "\n",
    "    Returns:\n",
    "        Union[Dict[str,int], Dict[int,str]]: dictionarys for lookup and\n",
    "            reverse lookup\n",
    "    \"\"\"\n",
    "\n",
    "    word2id, id2word = {}, {}\n",
    "    if not no_pad:\n",
    "        word2id['<pad>'] = len(word2id)\n",
    "        id2word[len(id2word)] = '<pad>'\n",
    "    if not no_unk:\n",
    "        word2id['<unk>'] = len(word2id)\n",
    "        id2word[len(id2word)] = '<unk>'\n",
    "    if bos_eos is True:\n",
    "        word2id['<s>'] = len(word2id)\n",
    "        id2word[len(id2word)] = '<s>'\n",
    "        word2id['</s>'] = len(word2id)\n",
    "        id2word[len(id2word)] = '</s>'\n",
    "    with open(vocab_path, 'r', encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            if separator in line:\n",
    "                word, idx = line.strip('\\r\\n').split(' '+separator+' ')\n",
    "                idx = int(idx)\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                idx = len(word2id)\n",
    "            if word not in word2id:\n",
    "                word2id[word] = idx\n",
    "                id2word[idx] = word\n",
    "    return word2id, id2word\n",
    "\n",
    "\n",
    "def read_vocab_from_data_file(\n",
    "    data_path: str,\n",
    "    vocab_config: Dict[str, Any] = None,\n",
    "    with_tag: bool = True,\n",
    "    separator: str = ':'\n",
    ") -> Union[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Build a vocab from a data file\n",
    "\n",
    "    Args:\n",
    "        data_path (str): file path of data\n",
    "        vocab_config (Dict[str, Any], optional): vocab config. Defaults to None.\n",
    "        with_tag (bool, optional): use tags. Defaults to True.\n",
    "        separator (_type_, optional): separator token to use. Defaults to ':'.\n",
    "\n",
    "    Returns:\n",
    "        Union[Dict[str, int], Dict[int, str]]: dictionarys for lookup and\n",
    "            reverse lookup\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_config is None:\n",
    "        vocab_config = {'mini_word_freq': 1,\n",
    "                        'bos_eos': False, 'lowercase': False}\n",
    "    print('Reading source data ...')\n",
    "    input_seqs = []\n",
    "    with open(data_path, 'r', encoding=\"utf8\") as file:\n",
    "        for _, line in enumerate(file):\n",
    "            slot_tag_line = line.strip('\\n\\r').split(' <=> ')[0]\n",
    "            if slot_tag_line == \"\":\n",
    "                continue\n",
    "            in_seq = []\n",
    "            for item in slot_tag_line.split(' '):\n",
    "                if with_tag:\n",
    "                    tmp = item.split(separator)\n",
    "                    word, _ = separator.join(tmp[:-1]), tmp[-1]\n",
    "                else:\n",
    "                    word = item\n",
    "                if vocab_config['lowercase']:\n",
    "                    word = word.lower()\n",
    "                in_seq.append(word)\n",
    "            input_seqs.append(in_seq)\n",
    "\n",
    "    print('Constructing input vocabulary from ', data_path, ' ...')\n",
    "    word2idx, idx2word = construct_vocab(input_seqs, vocab_config)\n",
    "    return (word2idx, idx2word)\n",
    "\n",
    "\n",
    "def read_seqtag_data_with_class(\n",
    "    data_path: str,\n",
    "    word2idx: Dict[str, int],\n",
    "    tag2idx: Dict[str, int],\n",
    "    class2idx: Dict[str, int],\n",
    "    separator: str = ':',\n",
    "    multi_class: bool = False,\n",
    "    keep_order: bool = False,\n",
    "    lowercase: bool = False\n",
    ") -> Union[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"Read data from files.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): file path of data\n",
    "        word2idx (Dict[str, int]): input vocab\n",
    "        tag2idx (Dict[str, int]): tag vocab\n",
    "        class2idx (Dict[str, int]): classification vocab\n",
    "        separator (_type_, optional): separator to use. Defaults to ':'.\n",
    "        multi_class (bool, optional): multiple classifiers. Defaults to False.\n",
    "        keep_order (bool, optional): keep a track of line number.\n",
    "            Defaults to False.\n",
    "        lowercase (bool, optional): use lowercase. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Union[Dict[str, Any], Dict[str, Any], Dict[str, Any]]: input features,\n",
    "            tag labels, class labels\n",
    "    \"\"\"\n",
    "\n",
    "    print('Reading source data ...')\n",
    "    input_seqs = []\n",
    "    tag_seqs = []\n",
    "    class_labels = []\n",
    "    line_num = -1\n",
    "    with open(data_path, 'r', encoding=\"utf8\") as file:\n",
    "        for _, line in enumerate(file):\n",
    "            line_num += 1\n",
    "            slot_tag_line, class_name = line.strip('\\n\\r').split(' <=> ')\n",
    "            if slot_tag_line == \"\":\n",
    "                continue\n",
    "            in_seq, tag_seq = [], []\n",
    "            for item in slot_tag_line.split(' '):\n",
    "                tmp = item.split(separator)\n",
    "                word, tag = separator.join(tmp[:-1]), tmp[-1]\n",
    "                if lowercase:\n",
    "                    word = word.lower()\n",
    "                in_seq.append(\n",
    "                    word2idx[word] if word in word2idx else word2idx['<unk>'])\n",
    "                tag_seq.append(tag2idx[tag] if tag in tag2idx else (\n",
    "                    tag2idx['<unk>'], tag))\n",
    "            if keep_order:\n",
    "                in_seq.append(line_num)\n",
    "            input_seqs.append(in_seq)\n",
    "            tag_seqs.append(tag_seq)\n",
    "            if multi_class:\n",
    "                if class_name == '':\n",
    "                    class_labels.append([])\n",
    "                else:\n",
    "                    class_labels.append([class2idx[val]\n",
    "                                        for val in class_name.split(';')])\n",
    "            else:\n",
    "                if ';' not in class_name:\n",
    "                    class_labels.append(class2idx[class_name])\n",
    "                else:\n",
    "                    # get the first class for training\n",
    "                    class_labels.append(\n",
    "                        (\n",
    "                            class2idx[class_name.split(';')[0]],\n",
    "                            class_name.split(';')\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    input_feats = {'data': input_seqs}\n",
    "    tag_labels = {'data': tag_seqs}\n",
    "    class_labels = {'data': class_labels}\n",
    "\n",
    "    return input_feats, tag_labels, class_labels\n",
    "\n",
    "\n",
    "class ATISDataset(Dataset):\n",
    "    \"\"\"Dataset for use within PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, sentences, tags, class_labels, tokenizer, max_length,\n",
    "            word2id, id2word,\n",
    "            class2id, id2class,\n",
    "            tag2id, id2tag):\n",
    "\n",
    "        self.len = len(sentences)\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.class_labels = class_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_length\n",
    "\n",
    "        self.word2id, self.id2word = word2id, id2word\n",
    "        self.class2id, self.id2class = class2id, id2class\n",
    "        self.tag2id, self.id2word = tag2id, id2tag\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        sentence = self.sentences[index].strip().split()\n",
    "        word_labels = self.tags[index]\n",
    "        class_label = self.class2id[self.class_labels[index]]\n",
    "\n",
    "        labels = [self.tag2id[label] for label in word_labels]\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  is_split_into_words=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len\n",
    "                                  )\n",
    "\n",
    "        encoded_labels = np.ones(\n",
    "            len(encoding['offset_mapping']), dtype=int) * -100\n",
    "\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding['offset_mapping']):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels, dtype=torch.long)\n",
    "        item['class_label'] = torch.as_tensor(class_label, dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def load_dataset(data_path, tokenizer, max_length):\n",
    "    \"\"\"load the dataset\n",
    "\n",
    "    Args:\n",
    "        data_path (str): _description_\n",
    "        tokenizer : transformers tokenizer_\n",
    "        max_length (int): max padding length\n",
    "    Returns:\n",
    "        Dict[str, Any] : collection of datasets\n",
    "    \"\"\"\n",
    "    word2id, id2word = read_vocab_from_data_file(data_path + \"/train\")\n",
    "    class2id, id2class = read_vocab_file(data_path + \"/vocab.intent\")\n",
    "    tag2id, id2tag = read_vocab_file(data_path + \"/vocab.slot\")\n",
    "\n",
    "    def get_ds(file_name):\n",
    "        input_feats, tag_labels, class_labels = read_seqtag_data_with_class(\n",
    "            data_path + \"/\" + file_name, word2id, tag2id, class2id)\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        cls_labels = []\n",
    "        for i in range(len(input_feats['data'])):\n",
    "            sent = input_feats['data'][i]\n",
    "            tag = tag_labels['data'][i]\n",
    "            class_label = class_labels['data'][i]\n",
    "            if not isinstance(class_label, int):\n",
    "                class_label = class_label[0]\n",
    "\n",
    "            sentences.append(\" \".join([id2word[idx] for idx in sent]))\n",
    "            labels.append([id2tag[idx] for idx in tag])\n",
    "            cls_labels.append(id2class[class_label])\n",
    "        return ATISDataset(sentences, labels, cls_labels,\n",
    "                           tokenizer, max_length,\n",
    "                           word2id, id2word,\n",
    "                           class2id, id2class,\n",
    "                           tag2id, id2tag)\n",
    "\n",
    "    return {\"train\": get_ds(\"train_all\"),\n",
    "            \"test\": get_ds(\"test\"),\n",
    "            \"word2id\": word2id, \"id2word\": id2word,\n",
    "            \"tag2id\": word2id, \"id2tag\": id2tag,\n",
    "            \"class2id\": word2id, \"id2class\": id2class}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d46adb-026e-4d51-bb5b-66b2493c6444",
   "metadata": {},
   "source": [
    "### 3.3 Model Definition\n",
    "\n",
    "We pull a pre-trained \"bert-base_uncased\" model from Hugging Face and add custom token and sequence classification heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ab064-7dd4-47e1-bfea-f7c354083a5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Copyright (C) 2022 Intel Corporation\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# pylint: disable=C0415,E0401,R0914\n",
    "\n",
    "\"\"\"\n",
    "Intent and Token Classification Model built using BERT\n",
    "\"\"\"\n",
    "\n",
    "class IntentAndTokenClassifier(torch.nn.Module):\n",
    "    \"\"\"Model that performs intent and token classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_token_labels: int,\n",
    "        num_sequence_labels: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_token_labels = num_token_labels\n",
    "        self.num_sequence_labels = num_sequence_labels\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.token_classifier = torch.nn.Linear(\n",
    "            768, self.num_token_labels)\n",
    "        self.sequence_classifier = torch.nn.Linear(\n",
    "            768,\n",
    "            self.num_sequence_labels\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        token_labels=None,\n",
    "        sequence_labels=None\n",
    "    ) -> None:\n",
    "        \"\"\"Predicts the intent and token tags for a given input sequence.\n",
    "\n",
    "        Args:\n",
    "            input_ids (optional): tokenized sentence. Defaults to None.\n",
    "            attention_mask (optional): attention mask to use. Defaults to None.\n",
    "            token_type_ids (optional): token ids. Defaults to None.\n",
    "            position_ids (optional): position ids. Defaults to None.\n",
    "            head_mask (optional): head mask. Defaults to None.\n",
    "            output_attentions (optional): whether to output attentions.\n",
    "                Defaults to None.\n",
    "            output_hidden_states (optional): whether to output hidden states.\n",
    "                Defaults to None.\n",
    "            token_labels (optional): true tag labels for each token to compute\n",
    "                loss. Defaults to None.\n",
    "            sequence_labels (optional): true class label to compute loss.\n",
    "                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            logits_token, token_loss, logits_sequence, sequence_loss\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "\n",
    "        token_output = outputs[0]\n",
    "\n",
    "        sequence_output = outputs[1]\n",
    "\n",
    "        logits_token = self.token_classifier(token_output)\n",
    "        logits_sequence = self.sequence_classifier(sequence_output)\n",
    "\n",
    "        token_loss = 0\n",
    "        if token_labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            token_loss = loss_fct(\n",
    "                logits_token.view(-1, self.num_token_labels),\n",
    "                token_labels.view(-1)\n",
    "            )\n",
    "\n",
    "        sequence_loss = 0\n",
    "        if sequence_labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            sequence_loss = loss_fct(\n",
    "                logits_sequence.view(-1, self.num_sequence_labels),\n",
    "                sequence_labels.view(-1)\n",
    "            )\n",
    "\n",
    "        if token_labels is not None and sequence_labels is not None:\n",
    "            return logits_token, token_loss, logits_sequence, sequence_loss\n",
    "        return logits_token, logits_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6d239-7916-429e-af59-cca747428d66",
   "metadata": {},
   "source": [
    "### 3.4 Evaluation Function\n",
    "\n",
    "Function takes in a trained model and test data loader. It performs inference on the hold-out dataset and returns NER and CLS accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b2382-e748-4374-994b-5d6d621a8ed4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Copyright (C) 2022 Intel Corporation\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# pylint: disable=C0415,E0401,R0914\n",
    "\n",
    "\"\"\"\n",
    "Train the intent and classfier model\n",
    "\"\"\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def evaluate_accuracy(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: IntentAndTokenClassifier,\n",
    ") -> Union[float, float]:\n",
    "    \"\"\"Evaluate the accuracy on the provided dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): dataloader to evaluate on\n",
    "        model (IntentAndTokenClassifier): model to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Union[float, float]: token prediction accuracy, class prediction\n",
    "            accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    tr_tk_preds, tr_tk_labels = [], []\n",
    "    tr_sq_preds, tr_sq_labels = [], []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(dataloader):\n",
    "\n",
    "            ids = batch['input_ids']\n",
    "            mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            class_label = batch['class_label']\n",
    "\n",
    "            # pass inputs through model\n",
    "            out = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_labels=labels,\n",
    "                sequence_labels=class_label)\n",
    "\n",
    "            tr_tk_logits = out[0]\n",
    "            tr_sq_logits = out[2]\n",
    "\n",
    "            # compute batch accuracy for token classification\n",
    "            flattened_targets = labels.view(-1)\n",
    "            active_logits = tr_tk_logits.view(-1, model.num_token_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "\n",
    "            # only get predictions of relevant tags\n",
    "            active_accuracy = labels.view(-1) != -100\n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(\n",
    "                flattened_predictions,\n",
    "                active_accuracy\n",
    "            )\n",
    "\n",
    "            tr_tk_labels.extend(labels.numpy())\n",
    "            tr_tk_preds.extend(predictions.numpy())\n",
    "\n",
    "            # compute accuracy for seqeunce classification\n",
    "            predictions_sq = torch.argmax(tr_sq_logits, axis=1)\n",
    "            tr_sq_labels.extend(class_label.numpy())\n",
    "            tr_sq_preds.extend(predictions_sq.numpy())\n",
    "\n",
    "    return (\n",
    "        accuracy_score(tr_tk_labels, tr_tk_preds),\n",
    "        accuracy_score(tr_sq_labels, tr_sq_preds)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a4ffb-fda3-497e-9ff6-5e3e3a266cd4",
   "metadata": {},
   "source": [
    "### 3.5 Model Training Function\n",
    "\n",
    "This function contains options for triggering AMP and bf16 OR training with AVX512 and FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326af06d-f1c4-4094-a3c2-60732b8d4141",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        model: torch.nn.Module,\n",
    "        epochs: int = 5,\n",
    "        amx: bool = True,\n",
    "        dataType: str = 'bf16',\n",
    "        max_grad_norm: float = 10) -> None:\n",
    "    \"\"\"train a model on the given dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): training dataset\n",
    "        model (torch.nn.Module): model to train\n",
    "        optimizer (torch.optim.Optimizer): optimizer to use\n",
    "        epochs (int, optional): number of training epochs. Defaults to 5.\n",
    "        max_grad_norm (float, optional): gradient clipping. Defaults to 10.\n",
    "    \"\"\"\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Configure environment variable\n",
    "    if not amx and 'bf16' == dataType:\n",
    "        print('going to AVX BF16 rather than AMX')\n",
    "        os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"AVX512_CORE_BF16\"\n",
    "    else:\n",
    "        os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"DEFAULT\"\n",
    "    \n",
    "    \n",
    "    # Optimize with BF16 or FP32 (default)\n",
    "    if \"bf16\" == dataType:\n",
    "        print('setting dtype to bf16 in IPEX')\n",
    "        model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)\n",
    "    else:\n",
    "        model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        running_loss = 0\n",
    "        tr_tk_preds, tr_tk_labels = [], []\n",
    "        tr_sq_preds, tr_sq_labels = [], []\n",
    "\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ids = batch['input_ids']\n",
    "            mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            class_label = batch['class_label']\n",
    "\n",
    "            # pass inputs through model\n",
    "            if \"bf16\" == dataType:\n",
    "                with torch.cpu.amp.autocast(): # required or Auto Mixed Precision (AMP)\n",
    "                    out = model(\n",
    "                        input_ids=ids,\n",
    "                        attention_mask=mask,\n",
    "                        token_labels=labels,\n",
    "                        sequence_labels=class_label)\n",
    "        \n",
    "                    # evaluate loss\n",
    "                    token_loss = out[1]\n",
    "                    sequence_loss = out[3]\n",
    "                    combined_loss = token_loss + sequence_loss\n",
    "        \n",
    "                    running_loss += combined_loss.item()\n",
    "        \n",
    "                    tr_tk_logits = out[0]\n",
    "                    tr_sq_logits = out[2]\n",
    "        \n",
    "                    if idx % 100 == 0:\n",
    "                        print(\"loss/100 batches: %.4f\", running_loss/(idx + 1))\n",
    "        \n",
    "                    # compute batch accuracy for token classification\n",
    "                    flattened_targets = labels.view(-1)\n",
    "                    active_logits = tr_tk_logits.view(-1, model.num_token_labels)\n",
    "                    flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "        \n",
    "                    # only get predictions of relevant tags\n",
    "                    active_accuracy = labels.view(-1) != -100\n",
    "                    labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "                    predictions = torch.masked_select(\n",
    "                        flattened_predictions,\n",
    "                        active_accuracy)\n",
    "        \n",
    "                    tr_tk_labels.extend(labels.numpy())\n",
    "                    tr_tk_preds.extend(predictions.numpy())\n",
    "        \n",
    "                    # compute accuracy for seqeunce classification\n",
    "                    predictions_sq = torch.argmax(tr_sq_logits, axis=1)\n",
    "                    tr_sq_labels.extend(class_label.numpy())\n",
    "                    tr_sq_preds.extend(predictions_sq.numpy())\n",
    "        \n",
    "                    # clip gradients for stability\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "                    combined_loss.backward()\n",
    "            else:\n",
    "                out = model(input_ids=ids,\n",
    "                            attention_mask=mask,\n",
    "                            token_labels=labels,\n",
    "                            sequence_labels=class_label)\n",
    "    \n",
    "                # evaluate loss\n",
    "                token_loss = out[1]\n",
    "                sequence_loss = out[3]\n",
    "                combined_loss = token_loss + sequence_loss\n",
    "    \n",
    "                running_loss += combined_loss.item()\n",
    "    \n",
    "                tr_tk_logits = out[0]\n",
    "                tr_sq_logits = out[2]\n",
    "    \n",
    "                if idx % 100 == 0:\n",
    "                    print(\"loss/100 batches: %.4f\", running_loss/(idx + 1))\n",
    "    \n",
    "                # compute batch accuracy for token classification\n",
    "                flattened_targets = labels.view(-1)\n",
    "                active_logits = tr_tk_logits.view(-1, model.num_token_labels)\n",
    "                flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "    \n",
    "                # only get predictions of relevant tags\n",
    "                active_accuracy = labels.view(-1) != -100\n",
    "                labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "                predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "    \n",
    "                tr_tk_labels.extend(labels.numpy())\n",
    "                tr_tk_preds.extend(predictions.numpy())\n",
    "    \n",
    "                # compute accuracy for seqeunce classification\n",
    "                predictions_sq = torch.argmax(tr_sq_logits, axis=1)\n",
    "                tr_sq_labels.extend(class_label.numpy())\n",
    "                tr_sq_preds.extend(predictions_sq.numpy())\n",
    "    \n",
    "                # clip gradients for stability\n",
    "                torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "    \n",
    "                combined_loss.backward()\n",
    "                    \n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(\"Training loss epoch #%d : %.4f\", epoch, epoch_loss)\n",
    "        print(\"Training NER accuracy epoch #%d : %.4f\", epoch, accuracy_score(tr_tk_labels, tr_tk_preds))\n",
    "        print(\"Training CLS accuracy epoch #%d : %.4f\", epoch, accuracy_score(tr_sq_labels, tr_sq_preds))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890b4dd-1e61-4c65-82fa-3bd122183c1e",
   "metadata": {},
   "source": [
    "### 3.6 Setting File Paths and Hyperparameters\n",
    "\n",
    "##### Setting File Paths:\n",
    "- **save_model_dir:** Location where models are saved\n",
    "- **data_path:** Path to data\n",
    "- **model_name:** name of model (.h5)\n",
    "\n",
    "##### Training parameters:\n",
    "- **MAX_LENGTH:** max padding length\n",
    "- **BATCH_SIZE:** batch size\n",
    "- **EPOCHS:** training epochs\n",
    "- **MAX_GRAD_NORM:** gradient normalization cut-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56abbd-8a0d-48fb-ae6c-0c19d88f1d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intel = True\n",
    "save_model_dir = ''\n",
    "data_path = ''\n",
    "model_name = ''\n",
    "\n",
    "# training parameters\n",
    "MAX_LENGTH = 64\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 3\n",
    "MAX_GRAD_NORM = 10\n",
    "\n",
    "# set RNG seed for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc15e6-c0a4-485f-b401-1a2d2c0e2c04",
   "metadata": {},
   "source": [
    "### 3.7 Tokenizing Dataset (Airline Travel Info Systems - ATIS)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5825b86-3513-4d83-a835-e1bf982302cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Read in the datasets and crate dataloaders\n",
    "print(\"Reading in the data...\")\n",
    "\n",
    "dataset = load_dataset(data_path, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset['train'], batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(dataset['test'], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58640d-0cb5-4857-8767-742d8e1ce3df",
   "metadata": {},
   "source": [
    "### 3.8 Model Training and Evaluation IPEX AMX and AMP (bf16 enabled)\n",
    "\n",
    "- Instantiate our model object\n",
    "- Train with Advanced Matrix Extensions activated and bfloat16 Auto Mixed Precision\n",
    "- Evaluate trained model against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c5e61-2e1c-4fd7-af0b-64e8b7476b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and prepare for training\n",
    "start_bf16_wAMX = time.time()\n",
    "model_bf16_wAMX = IntentAndTokenClassifier(\n",
    "    num_token_labels=len(dataset['train'].tag2id),\n",
    "    num_sequence_labels=len(dataset['train'].class2id)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "model_bf16_wAMX_trained = train(train_loader, model_bf16_wAMX, epochs=EPOCHS, max_grad_norm=MAX_GRAD_NORM, amx=True, dataType='bf16')\n",
    "training_time_bf16_wAMX = time.time()\n",
    "\n",
    "# Evaluate accuracy on the testing set in batches\n",
    "accuracy_ner_bf16_wAMX, accuracy_class_bf16_wAMX = evaluate_accuracy(test_loader, model_bf16_wAMX_trained)\n",
    "testing_time_bf16_wAMX = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90784fd3-0ecf-4fa1-a56b-9ebe4ae53593",
   "metadata": {},
   "source": [
    "### 3.9 Model Training and Evaluation of IPEX FP32 with AVX-512\n",
    "\n",
    "- Instantiate our model object\n",
    "- We switch from bfloat16 with AMX to FP32 with AVX-512. \n",
    "- Evaluate trained model against test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0077def-3297-412b-9a27-14197a05ba76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and prepare for training\n",
    "start_fp32 = time.time()\n",
    "model_fp32 = IntentAndTokenClassifier(\n",
    "    num_token_labels=len(dataset['train'].tag2id),\n",
    "    num_sequence_labels=len(dataset['train'].class2id)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "model_fp32_trained = train(train_loader, model_fp32, epochs=EPOCHS, max_grad_norm=MAX_GRAD_NORM, amx=False, dataType='fp32')\n",
    "training_time_fp32 = time.time()\n",
    "\n",
    "# Evaluate accuracy on the testing set in batches\n",
    "accuracy_ner_fp32, accuracy_class_fp32 = evaluate_accuracy(test_loader, model_fp32_trained)\n",
    "testing_time_fp32 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528fab99-3472-4842-960f-4ad82ccf0f9c",
   "metadata": {},
   "source": [
    "# 4. Calculating Accuracy Scores, Training, and Inference Times\n",
    "\n",
    "Let's calculate the NER and CLS accuracy scores, and the training, inference, and total times for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103d2c1-8265-4ea2-8782-e979a88be6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TIME METRICS')\n",
    "\n",
    "print(\"=======> FP32 ONLY - Test Accuracy on NER: \", accuracy_ner_fp32)\n",
    "print(\"=======> FP32 ONLY - Test Accuracy on CLS: \", accuracy_class_fp32)\n",
    "print(\"=======> FP32 ONLY - Training Time   mins\", (training_time_fp32 - start_fp32)/60)\n",
    "print(\"=======> FP32 ONLY - Inference Time:  secs\", (testing_time_fp32 - training_time_fp32))\n",
    "print(\"=======> FP32 ONLY - Total Time:  mins\", (testing_time_fp32 - start_fp32)/60)\n",
    "\n",
    "print('-'*100)\n",
    "\n",
    "print(\"=======> BF16 with AMX - Test Accuracy on NER : \", accuracy_ner_bf16_wAMX)\n",
    "print(\"=======> BF16 with AMX - Test Accuracy on CLS : \", accuracy_class_bf16_wAMX)\n",
    "print(\"=======> BF16 with AMX - Training Time:  mins\", (training_time_bf16_wAMX - start_bf16_wAMX)/60)\n",
    "print(\"=======> BF16 with AMX - Inference Time:  secs\", (testing_time_bf16_wAMX - training_time_bf16_wAMX))\n",
    "print(\"=======> BF16 with AMX - Total Time:  mins\", (testing_time_bf16_wAMX - start_bf16_wAMX)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc7c9b-0107-487e-b39f-d7ea88e47b3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Conclusion and Summary\n",
    "\n",
    "AMX and AMP half-precision training with bfloat16 are powerful techniques that make CPUs a competitive choice for fine-tuning large language models (LLMs). By leveraging optimized software tools like Intel Extension for PyTorch in tandem with 4th Generation Xeon CPUs, developers can achieve faster training times with minimal sacrifices in accuracy, all while reducing the cost of hardware infrastructure.\n",
    "\n",
    "This has significant implications for the chatbot industry, as CPUs can now be considered a viable alternative to GPUs for chatbot training and deployment. Furthermore, with the availability of AMX and AMP, developers can now take advantage of the capabilities of CPUs, such as more significant memory, higher core memory capacity, and TMULs for more extensive matrix computations.\n",
    "\n",
    "In short, these techniques have the potential to revolutionize the chatbot industry, making it easier and more cost-effective for teams of all sizes to develop and deploy chatbots that deliver exceptional customer experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b601471-bfe4-417a-86db-3d5f204b61ed",
   "metadata": {},
   "source": [
    "*Disclaimer: Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. © Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idp",
   "language": "python",
   "name": "idp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
