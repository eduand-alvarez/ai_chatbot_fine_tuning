{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82dac0-5db8-426c-90d2-3c855df20ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92bee0-50aa-45b2-977f-5759c3e85c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gives control over the number of threads \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "#os.environ.pop('OMP_NUM_THREADS', None)\n",
    "#torch.set_num_threads(\n",
    "\n",
    "# manages the amount of time before cores go back to sleep between processes\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"50\"\n",
    "#os.environ.pop('KMP_BLOCKTIME', None)\n",
    "\n",
    "# We can specify how many threads to use in total and how the threads are distributed across different layers of machine topology\n",
    "#os.environ[\"KMP_HW_SUBSET\"] = \"1s,1n,56c,2t\"\n",
    "os.environ.pop('KMP_HW_SUBSET', None)\n",
    "\n",
    "# Thread affinity restricts execution of certain threads (virtual execution units) to a subset of the physical processing units in a multiprocessor computer. \n",
    "#os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
    "os.environ.pop('KMP_AFFINITY', None)\n",
    "\n",
    "os.environ[\"MKLDNN_VERBOSE\"] = \"0\"\n",
    "\n",
    "!echo $OMP_NUM_THREADS\n",
    "!echo $KMP_BLOCKTIME\n",
    "!echo $KMP_HW_SUBSET\n",
    "!echo $KMP_AFFINITY\n",
    "\n",
    "\n",
    "# use top + 1 + t , to check the utilization of your cores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65d36d-d0de-4ed4-87db-701998fd6f34",
   "metadata": {},
   "source": [
    "# AI-Powered Customer Care Chatbots (based on Intel AI Reference Kit)  \n",
    "\n",
    "Briefly, given a customer query, the AI system must understand the intent and the entities involved within the query, lookup or launch the relevant information, and return the appropriate response to the customer in a reasonable amount of time. In this example, we focus on leveraging the IntelÂ® oneAPI AI Analytics Toolkit on the task of training and deploying an accurate and quick AI system to predict the Intent and Entities of a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd84957-be2d-4b07-bade-1279292e201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# if using intel, optimize the model and the optimizer\n",
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169ddc8-c291-4936-8a8f-3a3547bce35a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-1s,\n",
    "\n",
    "# Copyright (C) 2022 Intel Corporation\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# pylint: disable=C0415,E0401,R0914\n",
    "\n",
    "\"\"\"\n",
    "Code adopted from\n",
    "https://github.com/sz128/slot_filling_and_intent_detection_of_SLU\n",
    "\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def construct_vocab(\n",
    "    input_seqs: List[str],\n",
    "    vocab_config: Dict[str, Any] = None\n",
    ") -> Union[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Construct a vocabulary given a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        input_seqs (List[str]): list of sentences\n",
    "        vocab_config (Dict[str, Any], optional): options for constructing\n",
    "            the vocab. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Union[Dict[str,int], Dict[int, str]]: dictionarys for lookup and\n",
    "            reverse lookup\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_config is None:\n",
    "        vocab_config = {'mini_word_freq': 1, 'bos_eos': False}\n",
    "\n",
    "    vocab = {}\n",
    "    for seq in input_seqs:\n",
    "        if isinstance(seq, type([])):\n",
    "            for word in seq:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] += 1\n",
    "        else:\n",
    "            if seq not in vocab:\n",
    "                vocab[seq] = 1\n",
    "            else:\n",
    "                vocab[seq] += 1\n",
    "\n",
    "    # Discard start, end, pad and unk tokens if already present\n",
    "    if '<s>' in vocab:\n",
    "        del vocab['<s>']\n",
    "    if '<pad>' in vocab:\n",
    "        del vocab['<pad>']\n",
    "    if '</s>' in vocab:\n",
    "        del vocab['</s>']\n",
    "    if '<unk>' in vocab:\n",
    "        del vocab['<unk>']\n",
    "\n",
    "    if vocab_config['bos_eos'] is True:\n",
    "        word2id = {'<pad>': 0, '<unk>': 1, '<s>': 2, '</s>': 3}\n",
    "        id2word = {0: '<pad>', 1: '<unk>', 2: '<s>', 3: '</s>'}\n",
    "    else:\n",
    "        word2id = {'<pad>': 0, '<unk>': 1, }\n",
    "        id2word = {0: '<pad>', 1: '<unk>', }\n",
    "\n",
    "    sorted_word2id = sorted(\n",
    "        vocab.items(),\n",
    "        key=operator.itemgetter(1),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    sorted_words = [x[0] for x in sorted_word2id if x[1]\n",
    "                    >= vocab_config['mini_word_freq']]\n",
    "\n",
    "    for word in sorted_words:\n",
    "        idx = len(word2id)\n",
    "        word2id[word] = idx\n",
    "        id2word[idx] = word\n",
    "\n",
    "    return word2id, id2word\n",
    "\n",
    "\n",
    "def read_vocab_file(\n",
    "        vocab_path: str,\n",
    "        bos_eos: bool = False,\n",
    "        no_pad: bool = False,\n",
    "        no_unk: bool = False,\n",
    "        separator: str = ':'\n",
    ") -> Union[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Reads a pre-existing vocabulary.\n",
    "\n",
    "    Args:\n",
    "        vocab_path (str): path to vocab file\n",
    "        bos_eos (bool, optional): add begining and ending. Defaults to False.\n",
    "        no_pad (bool, optional): use pad tokens. Defaults to False.\n",
    "        no_unk (bool, optional): use unknown tokens. Defaults to False.\n",
    "        separator (str, optional): separator token  to use. Defaults to ':'.\n",
    "\n",
    "    Returns:\n",
    "        Union[Dict[str,int], Dict[int,str]]: dictionarys for lookup and\n",
    "            reverse lookup\n",
    "    \"\"\"\n",
    "\n",
    "    word2id, id2word = {}, {}\n",
    "    if not no_pad:\n",
    "        word2id['<pad>'] = len(word2id)\n",
    "        id2word[len(id2word)] = '<pad>'\n",
    "    if not no_unk:\n",
    "        word2id['<unk>'] = len(word2id)\n",
    "        id2word[len(id2word)] = '<unk>'\n",
    "    if bos_eos is True:\n",
    "        word2id['<s>'] = len(word2id)\n",
    "        id2word[len(id2word)] = '<s>'\n",
    "        word2id['</s>'] = len(word2id)\n",
    "        id2word[len(id2word)] = '</s>'\n",
    "    with open(vocab_path, 'r', encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            if separator in line:\n",
    "                word, idx = line.strip('\\r\\n').split(' '+separator+' ')\n",
    "                idx = int(idx)\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                idx = len(word2id)\n",
    "            if word not in word2id:\n",
    "                word2id[word] = idx\n",
    "                id2word[idx] = word\n",
    "    return word2id, id2word\n",
    "\n",
    "\n",
    "def read_vocab_from_data_file(\n",
    "    data_path: str,\n",
    "    vocab_config: Dict[str, Any] = None,\n",
    "    with_tag: bool = True,\n",
    "    separator: str = ':'\n",
    ") -> Union[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Build a vocab from a data file\n",
    "\n",
    "    Args:\n",
    "        data_path (str): file path of data\n",
    "        vocab_config (Dict[str, Any], optional): vocab config. Defaults to None.\n",
    "        with_tag (bool, optional): use tags. Defaults to True.\n",
    "        separator (_type_, optional): separator token to use. Defaults to ':'.\n",
    "\n",
    "    Returns:\n",
    "        Union[Dict[str, int], Dict[int, str]]: dictionarys for lookup and\n",
    "            reverse lookup\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_config is None:\n",
    "        vocab_config = {'mini_word_freq': 1,\n",
    "                        'bos_eos': False, 'lowercase': False}\n",
    "    print('Reading source data ...')\n",
    "    input_seqs = []\n",
    "    with open(data_path, 'r', encoding=\"utf8\") as file:\n",
    "        for _, line in enumerate(file):\n",
    "            slot_tag_line = line.strip('\\n\\r').split(' <=> ')[0]\n",
    "            if slot_tag_line == \"\":\n",
    "                continue\n",
    "            in_seq = []\n",
    "            for item in slot_tag_line.split(' '):\n",
    "                if with_tag:\n",
    "                    tmp = item.split(separator)\n",
    "                    word, _ = separator.join(tmp[:-1]), tmp[-1]\n",
    "                else:\n",
    "                    word = item\n",
    "                if vocab_config['lowercase']:\n",
    "                    word = word.lower()\n",
    "                in_seq.append(word)\n",
    "            input_seqs.append(in_seq)\n",
    "\n",
    "    print('Constructing input vocabulary from ', data_path, ' ...')\n",
    "    word2idx, idx2word = construct_vocab(input_seqs, vocab_config)\n",
    "    return (word2idx, idx2word)\n",
    "\n",
    "\n",
    "def read_seqtag_data_with_class(\n",
    "    data_path: str,\n",
    "    word2idx: Dict[str, int],\n",
    "    tag2idx: Dict[str, int],\n",
    "    class2idx: Dict[str, int],\n",
    "    separator: str = ':',\n",
    "    multi_class: bool = False,\n",
    "    keep_order: bool = False,\n",
    "    lowercase: bool = False\n",
    ") -> Union[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"Read data from files.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): file path of data\n",
    "        word2idx (Dict[str, int]): input vocab\n",
    "        tag2idx (Dict[str, int]): tag vocab\n",
    "        class2idx (Dict[str, int]): classification vocab\n",
    "        separator (_type_, optional): separator to use. Defaults to ':'.\n",
    "        multi_class (bool, optional): multiple classifiers. Defaults to False.\n",
    "        keep_order (bool, optional): keep a track of line number.\n",
    "            Defaults to False.\n",
    "        lowercase (bool, optional): use lowercase. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Union[Dict[str, Any], Dict[str, Any], Dict[str, Any]]: input features,\n",
    "            tag labels, class labels\n",
    "    \"\"\"\n",
    "\n",
    "    print('Reading source data ...')\n",
    "    input_seqs = []\n",
    "    tag_seqs = []\n",
    "    class_labels = []\n",
    "    line_num = -1\n",
    "    with open(data_path, 'r', encoding=\"utf8\") as file:\n",
    "        for _, line in enumerate(file):\n",
    "            line_num += 1\n",
    "            slot_tag_line, class_name = line.strip('\\n\\r').split(' <=> ')\n",
    "            if slot_tag_line == \"\":\n",
    "                continue\n",
    "            in_seq, tag_seq = [], []\n",
    "            for item in slot_tag_line.split(' '):\n",
    "                tmp = item.split(separator)\n",
    "                word, tag = separator.join(tmp[:-1]), tmp[-1]\n",
    "                if lowercase:\n",
    "                    word = word.lower()\n",
    "                in_seq.append(\n",
    "                    word2idx[word] if word in word2idx else word2idx['<unk>'])\n",
    "                tag_seq.append(tag2idx[tag] if tag in tag2idx else (\n",
    "                    tag2idx['<unk>'], tag))\n",
    "            if keep_order:\n",
    "                in_seq.append(line_num)\n",
    "            input_seqs.append(in_seq)\n",
    "            tag_seqs.append(tag_seq)\n",
    "            if multi_class:\n",
    "                if class_name == '':\n",
    "                    class_labels.append([])\n",
    "                else:\n",
    "                    class_labels.append([class2idx[val]\n",
    "                                        for val in class_name.split(';')])\n",
    "            else:\n",
    "                if ';' not in class_name:\n",
    "                    class_labels.append(class2idx[class_name])\n",
    "                else:\n",
    "                    # get the first class for training\n",
    "                    class_labels.append(\n",
    "                        (\n",
    "                            class2idx[class_name.split(';')[0]],\n",
    "                            class_name.split(';')\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    input_feats = {'data': input_seqs}\n",
    "    tag_labels = {'data': tag_seqs}\n",
    "    class_labels = {'data': class_labels}\n",
    "\n",
    "    return input_feats, tag_labels, class_labels\n",
    "\n",
    "\n",
    "class ATISDataset(Dataset):\n",
    "    \"\"\"Dataset for use within PyTorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, sentences, tags, class_labels, tokenizer, max_length,\n",
    "            word2id, id2word,\n",
    "            class2id, id2class,\n",
    "            tag2id, id2tag):\n",
    "\n",
    "        self.len = len(sentences)\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.class_labels = class_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_length\n",
    "\n",
    "        self.word2id, self.id2word = word2id, id2word\n",
    "        self.class2id, self.id2class = class2id, id2class\n",
    "        self.tag2id, self.id2word = tag2id, id2tag\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        sentence = self.sentences[index].strip().split()\n",
    "        word_labels = self.tags[index]\n",
    "        class_label = self.class2id[self.class_labels[index]]\n",
    "\n",
    "        labels = [self.tag2id[label] for label in word_labels]\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  is_split_into_words=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len\n",
    "                                  )\n",
    "\n",
    "        encoded_labels = np.ones(\n",
    "            len(encoding['offset_mapping']), dtype=int) * -100\n",
    "\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding['offset_mapping']):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels, dtype=torch.long)\n",
    "        item['class_label'] = torch.as_tensor(class_label, dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def load_dataset(data_path, tokenizer, max_length):\n",
    "    \"\"\"load the dataset\n",
    "\n",
    "    Args:\n",
    "        data_path (str): _description_\n",
    "        tokenizer : transformers tokenizer_\n",
    "        max_length (int): max padding length\n",
    "    Returns:\n",
    "        Dict[str, Any] : collection of datasets\n",
    "    \"\"\"\n",
    "    word2id, id2word = read_vocab_from_data_file(data_path + \"/train\")\n",
    "    class2id, id2class = read_vocab_file(data_path + \"/vocab.intent\")\n",
    "    tag2id, id2tag = read_vocab_file(data_path + \"/vocab.slot\")\n",
    "\n",
    "    def get_ds(file_name):\n",
    "        input_feats, tag_labels, class_labels = read_seqtag_data_with_class(\n",
    "            data_path + \"/\" + file_name, word2id, tag2id, class2id)\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        cls_labels = []\n",
    "        for i in range(len(input_feats['data'])):\n",
    "            sent = input_feats['data'][i]\n",
    "            tag = tag_labels['data'][i]\n",
    "            class_label = class_labels['data'][i]\n",
    "            if not isinstance(class_label, int):\n",
    "                class_label = class_label[0]\n",
    "\n",
    "            sentences.append(\" \".join([id2word[idx] for idx in sent]))\n",
    "            labels.append([id2tag[idx] for idx in tag])\n",
    "            cls_labels.append(id2class[class_label])\n",
    "        return ATISDataset(sentences, labels, cls_labels,\n",
    "                           tokenizer, max_length,\n",
    "                           word2id, id2word,\n",
    "                           class2id, id2class,\n",
    "                           tag2id, id2tag)\n",
    "\n",
    "    return {\"train\": get_ds(\"train_all\"),\n",
    "            \"test\": get_ds(\"test\"),\n",
    "            \"word2id\": word2id, \"id2word\": id2word,\n",
    "            \"tag2id\": word2id, \"id2tag\": id2tag,\n",
    "            \"class2id\": word2id, \"id2class\": id2class}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ab064-7dd4-47e1-bfea-f7c354083a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Copyright (C) 2022 Intel Corporation\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# pylint: disable=C0415,E0401,R0914\n",
    "\n",
    "\"\"\"\n",
    "Intent and Token Classification Model built using BERT\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "class IntentAndTokenClassifier(torch.nn.Module):\n",
    "    \"\"\"Model that performs intent and token classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_token_labels: int,\n",
    "        num_sequence_labels: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_token_labels = num_token_labels\n",
    "        self.num_sequence_labels = num_sequence_labels\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.token_classifier = torch.nn.Linear(\n",
    "            768, self.num_token_labels)\n",
    "        self.sequence_classifier = torch.nn.Linear(\n",
    "            768,\n",
    "            self.num_sequence_labels\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        token_labels=None,\n",
    "        sequence_labels=None\n",
    "    ) -> None:\n",
    "        \"\"\"Predicts the intent and token tags for a given input sequence.\n",
    "\n",
    "        Args:\n",
    "            input_ids (optional): tokenized sentence. Defaults to None.\n",
    "            attention_mask (optional): attention mask to use. Defaults to None.\n",
    "            token_type_ids (optional): token ids. Defaults to None.\n",
    "            position_ids (optional): position ids. Defaults to None.\n",
    "            head_mask (optional): head mask. Defaults to None.\n",
    "            output_attentions (optional): whether to output attentions.\n",
    "                Defaults to None.\n",
    "            output_hidden_states (optional): whether to output hidden states.\n",
    "                Defaults to None.\n",
    "            token_labels (optional): true tag labels for each token to compute\n",
    "                loss. Defaults to None.\n",
    "            sequence_labels (optional): true class label to compute loss.\n",
    "                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            logits_token, token_loss, logits_sequence, sequence_loss\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "\n",
    "        token_output = outputs[0]\n",
    "\n",
    "        sequence_output = outputs[1]\n",
    "\n",
    "        logits_token = self.token_classifier(token_output)\n",
    "        logits_sequence = self.sequence_classifier(sequence_output)\n",
    "\n",
    "        token_loss = 0\n",
    "        if token_labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            token_loss = loss_fct(\n",
    "                logits_token.view(-1, self.num_token_labels),\n",
    "                token_labels.view(-1)\n",
    "            )\n",
    "\n",
    "        sequence_loss = 0\n",
    "        if sequence_labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            sequence_loss = loss_fct(\n",
    "                logits_sequence.view(-1, self.num_sequence_labels),\n",
    "                sequence_labels.view(-1)\n",
    "            )\n",
    "\n",
    "        if token_labels is not None and sequence_labels is not None:\n",
    "            return logits_token, token_loss, logits_sequence, sequence_loss\n",
    "        return logits_token, logits_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b2382-e748-4374-994b-5d6d621a8ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Copyright (C) 2022 Intel Corporation\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# pylint: disable=C0415,E0401,R0914\n",
    "\n",
    "\"\"\"\n",
    "Train the intent and classfier model\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import Union\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def evaluate_accuracy(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: IntentAndTokenClassifier,\n",
    ") -> Union[float, float]:\n",
    "    \"\"\"Evaluate the accuracy on the provided dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): dataloader to evaluate on\n",
    "        model (IntentAndTokenClassifier): model to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Union[float, float]: token prediction accuracy, class prediction\n",
    "            accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    tr_tk_preds, tr_tk_labels = [], []\n",
    "    tr_sq_preds, tr_sq_labels = [], []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(dataloader):\n",
    "\n",
    "            ids = batch['input_ids']\n",
    "            mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            class_label = batch['class_label']\n",
    "\n",
    "            # pass inputs through model\n",
    "            out = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_labels=labels,\n",
    "                sequence_labels=class_label)\n",
    "\n",
    "            tr_tk_logits = out[0]\n",
    "            tr_sq_logits = out[2]\n",
    "\n",
    "            # compute batch accuracy for token classification\n",
    "            flattened_targets = labels.view(-1)\n",
    "            active_logits = tr_tk_logits.view(-1, model.num_token_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "\n",
    "            # only get predictions of relevant tags\n",
    "            active_accuracy = labels.view(-1) != -100\n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(\n",
    "                flattened_predictions,\n",
    "                active_accuracy\n",
    "            )\n",
    "\n",
    "            tr_tk_labels.extend(labels.numpy())\n",
    "            tr_tk_preds.extend(predictions.numpy())\n",
    "\n",
    "            # compute accuracy for seqeunce classification\n",
    "            predictions_sq = torch.argmax(tr_sq_logits, axis=1)\n",
    "            tr_sq_labels.extend(class_label.numpy())\n",
    "            tr_sq_preds.extend(predictions_sq.numpy())\n",
    "\n",
    "    return (\n",
    "        accuracy_score(tr_tk_labels, tr_tk_preds),\n",
    "        accuracy_score(tr_sq_labels, tr_sq_preds)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a4ffb-fda3-497e-9ff6-5e3e3a266cd4",
   "metadata": {},
   "source": [
    "# Model Training Function\n",
    "\n",
    "This function contains options for triggering AMP and bf16 OR training with AVX512 and FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326af06d-f1c4-4094-a3c2-60732b8d4141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        model: torch.nn.Module,\n",
    "        epochs: int = 5,\n",
    "        amx: bool = True,\n",
    "        dataType: str = 'bf16',\n",
    "        max_grad_norm: float = 10) -> None:\n",
    "    \"\"\"train a model on the given dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): training dataset\n",
    "        model (torch.nn.Module): model to train\n",
    "        optimizer (torch.optim.Optimizer): optimizer to use\n",
    "        epochs (int, optional): number of training epochs. Defaults to 5.\n",
    "        max_grad_norm (float, optional): gradient clipping. Defaults to 10.\n",
    "    \"\"\"\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Configure environment variable\n",
    "    if not amx and 'bf16' == dataType:\n",
    "        print('going to AVX BF16 rather than AMX')\n",
    "        os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"AVX512_CORE_BF16\"\n",
    "    else:\n",
    "        os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"DEFAULT\"\n",
    "    \n",
    "    \n",
    "    # Optimize with BF16 or FP32 (default)\n",
    "    if \"bf16\" == dataType:\n",
    "        print('setting dtype to bf16 in IPEX')\n",
    "        model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)\n",
    "    else:\n",
    "        model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        running_loss = 0\n",
    "        tr_tk_preds, tr_tk_labels = [], []\n",
    "        tr_sq_preds, tr_sq_labels = [], []\n",
    "\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ids = batch['input_ids']\n",
    "            mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            class_label = batch['class_label']\n",
    "\n",
    "            # pass inputs through model\n",
    "            if \"bf16\" == dataType:\n",
    "                with torch.cpu.amp.autocast(): # required or Auto Mixed Precision (AMP)\n",
    "                    out = model(\n",
    "                        input_ids=ids,\n",
    "                        attention_mask=mask,\n",
    "                        token_labels=labels,\n",
    "                        sequence_labels=class_label)\n",
    "        \n",
    "                    # evaluate loss\n",
    "                    token_loss = out[1]\n",
    "                    sequence_loss = out[3]\n",
    "                    combined_loss = token_loss + sequence_loss\n",
    "        \n",
    "                    running_loss += combined_loss.item()\n",
    "        \n",
    "                    tr_tk_logits = out[0]\n",
    "                    tr_sq_logits = out[2]\n",
    "        \n",
    "                    if idx % 100 == 0:\n",
    "                        print(\"loss/100 batches: %.4f\", running_loss/(idx + 1))\n",
    "        \n",
    "                    # compute batch accuracy for token classification\n",
    "                    flattened_targets = labels.view(-1)\n",
    "                    active_logits = tr_tk_logits.view(-1, model.num_token_labels)\n",
    "                    flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "        \n",
    "                    # only get predictions of relevant tags\n",
    "                    active_accuracy = labels.view(-1) != -100\n",
    "                    labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "                    predictions = torch.masked_select(\n",
    "                        flattened_predictions,\n",
    "                        active_accuracy)\n",
    "        \n",
    "                    tr_tk_labels.extend(labels.numpy())\n",
    "                    tr_tk_preds.extend(predictions.numpy())\n",
    "        \n",
    "                    # compute accuracy for seqeunce classification\n",
    "                    predictions_sq = torch.argmax(tr_sq_logits, axis=1)\n",
    "                    tr_sq_labels.extend(class_label.numpy())\n",
    "                    tr_sq_preds.extend(predictions_sq.numpy())\n",
    "        \n",
    "                    # clip gradients for stability\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "                    combined_loss.backward()\n",
    "            else:\n",
    "                out = model(input_ids=ids,\n",
    "                            attention_mask=mask,\n",
    "                            token_labels=labels,\n",
    "                            sequence_labels=class_label)\n",
    "    \n",
    "                # evaluate loss\n",
    "                token_loss = out[1]\n",
    "                sequence_loss = out[3]\n",
    "                combined_loss = token_loss + sequence_loss\n",
    "    \n",
    "                running_loss += combined_loss.item()\n",
    "    \n",
    "                tr_tk_logits = out[0]\n",
    "                tr_sq_logits = out[2]\n",
    "    \n",
    "                if idx % 100 == 0:\n",
    "                    print(\"loss/100 batches: %.4f\", running_loss/(idx + 1))\n",
    "    \n",
    "                # compute batch accuracy for token classification\n",
    "                flattened_targets = labels.view(-1)\n",
    "                active_logits = tr_tk_logits.view(-1, model.num_token_labels)\n",
    "                flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "    \n",
    "                # only get predictions of relevant tags\n",
    "                active_accuracy = labels.view(-1) != -100\n",
    "                labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "                predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "    \n",
    "                tr_tk_labels.extend(labels.numpy())\n",
    "                tr_tk_preds.extend(predictions.numpy())\n",
    "    \n",
    "                # compute accuracy for seqeunce classification\n",
    "                predictions_sq = torch.argmax(tr_sq_logits, axis=1)\n",
    "                tr_sq_labels.extend(class_label.numpy())\n",
    "                tr_sq_preds.extend(predictions_sq.numpy())\n",
    "    \n",
    "                # clip gradients for stability\n",
    "                torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "    \n",
    "                combined_loss.backward()\n",
    "                    \n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(\"Training loss epoch #%d : %.4f\", epoch, epoch_loss)\n",
    "        print(\"Training NER accuracy epoch #%d : %.4f\", epoch, accuracy_score(tr_tk_labels, tr_tk_preds))\n",
    "        print(\"Training CLS accuracy epoch #%d : %.4f\", epoch, accuracy_score(tr_sq_labels, tr_sq_preds))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a7cce-edc1-4016-b2c1-33137532ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        model: torch.nn.Module,\n",
    "        epochs: int = 5,\n",
    "        amx: bool = True,\n",
    "        dataType: str = 'bf16',\n",
    "        max_grad_norm: float = 10) -> None:\n",
    "    \"\"\"train a model on the given dataset\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): training dataset\n",
    "        model (torch.nn.Module): model to train\n",
    "        optimizer (torch.optim.Optimizer): optimizer to use\n",
    "        epochs (int, optional): number of training epochs. Defaults to 5.\n",
    "        max_grad_norm (float, optional): gradient clipping. Defaults to 10.\n",
    "    \"\"\"\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Configure environment variable\n",
    "    if not amx and 'bf16' == dataType:\n",
    "        print('going to AVX BF16 rather than AMX')\n",
    "        os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"AVX512_CORE_BF16\"\n",
    "    else:\n",
    "        os.environ[\"ONEDNN_MAX_CPU_ISA\"] = \"DEFAULT\"\n",
    "    \n",
    "    \n",
    "    # Optimize with BF16 or FP32 (default)\n",
    "    if \"bf16\" == dataType:\n",
    "        print('setting dtype to bf16 in IPEX')\n",
    "        model, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)\n",
    "    else:\n",
    "        model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        running_loss = 0\n",
    "        tr_tk_preds, tr_tk_labels = [], []\n",
    "        tr_sq_preds, tr_sq_labels = [], []\n",
    "\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ids = batch['input_ids']\n",
    "            mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            class_label = batch['class_label']\n",
    "\n",
    "            # pass inputs through model\n",
    "            \n",
    "                with torch.cpu.amp.autocast(): # required or Auto Mixed Precision (AMP)\n",
    "            out = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_labels=labels,\n",
    "                sequence_labels=class_label)\n",
    "        \n",
    "            # evaluate loss\n",
    "            token_loss = out[1]\n",
    "            sequence_loss = out[3]\n",
    "            combined_loss = token_loss + sequence_loss\n",
    "        \n",
    "            running_loss += combined_loss.item()\n",
    "        \n",
    "            tr_tk_logits = out[0]\n",
    "            tr_sq_logits = out[2]\n",
    "        \n",
    "            if idx % 100 == 0:\n",
    "                print(\"loss/100 batches: %.4f\", running_loss/(idx + 1))\n",
    "        \n",
    "            # compute batch accuracy for token classification\n",
    "            flattened_targets = labels.view(-1)\n",
    "            active_logits = tr_tk_logits.view(-1, model.num_token_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "        \n",
    "            # only get predictions of relevant tags\n",
    "            active_accuracy = labels.view(-1) != -100\n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(\n",
    "                flattened_predictions,\n",
    "                active_accuracy)\n",
    "        \n",
    "            tr_tk_labels.extend(labels.numpy())\n",
    "            tr_tk_preds.extend(predictions.numpy())\n",
    "        \n",
    "            # compute accuracy for seqeunce classification\n",
    "            predictions_sq = torch.argmax(tr_sq_logits, axis=1)\n",
    "            tr_sq_labels.extend(class_label.numpy())\n",
    "            tr_sq_preds.extend(predictions_sq.numpy())\n",
    "        \n",
    "            # clip gradients for stability\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "            combined_loss.backward()\n",
    "            \n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(\"Training loss epoch #%d : %.4f\", epoch, epoch_loss)\n",
    "        print(\"Training NER accuracy epoch #%d : %.4f\", epoch, accuracy_score(tr_tk_labels, tr_tk_preds))\n",
    "        print(\"Training CLS accuracy epoch #%d : %.4f\", epoch, accuracy_score(tr_sq_labels, tr_sq_preds))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890b4dd-1e61-4c65-82fa-3bd122183c1e",
   "metadata": {},
   "source": [
    "## Setting some basic File Paths and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56abbd-8a0d-48fb-ae6c-0c19d88f1d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intel = True\n",
    "save_model_dir = './customer-chatbot/model/'\n",
    "data_path = './customer-chatbot/data/atis-2/'\n",
    "model_name = 'intel_ipex.pt'\n",
    "\n",
    "# training parameters\n",
    "MAX_LENGTH = 64\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 3\n",
    "MAX_GRAD_NORM = 10\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc15e6-c0a4-485f-b401-1a2d2c0e2c04",
   "metadata": {},
   "source": [
    "## Tokenizing Dataset (Airline Travel Info Systems - ATIS)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5825b86-3513-4d83-a835-e1bf982302cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Read in the datasets and crate dataloaders\n",
    "print(\"Reading in the data...\")\n",
    "\n",
    "dataset = load_dataset(data_path, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset['train'], batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(dataset['test'], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da373737-606d-4821-8522-2e88963f6496",
   "metadata": {},
   "source": [
    "# Example of Model Training accross various Configurations\n",
    "\n",
    "I've also included comparisons with: \n",
    "- bf16 with AMX\n",
    "- FP32 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58640d-0cb5-4857-8767-742d8e1ce3df",
   "metadata": {},
   "source": [
    "### IPEX bf16 w/ AMX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c5e61-2e1c-4fd7-af0b-64e8b7476b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and prepare for training\n",
    "start_bf16_wAMX = time.time()\n",
    "model_bf16_wAMX = IntentAndTokenClassifier(\n",
    "    num_token_labels=len(dataset['train'].tag2id),\n",
    "    num_sequence_labels=len(dataset['train'].class2id)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "model_bf16_wAMX_trained = train(train_loader, model_bf16_wAMX, epochs=EPOCHS, max_grad_norm=MAX_GRAD_NORM, amx=True, dataType='bf16')\n",
    "training_time_bf16_wAMX = time.time()\n",
    "\n",
    "# Evaluate accuracy on the testing set in batches\n",
    "accuracy_ner_bf16_wAMX, accuracy_class_bf16_wAMX = evaluate_accuracy(test_loader, model_bf16_wAMX_trained)\n",
    "testing_time_bf16_wAMX = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90784fd3-0ecf-4fa1-a56b-9ebe4ae53593",
   "metadata": {},
   "source": [
    "### IPEX FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0077def-3297-412b-9a27-14197a05ba76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model and prepare for training\n",
    "start_fp32 = time.time()\n",
    "model_fp32 = IntentAndTokenClassifier(\n",
    "    num_token_labels=len(dataset['train'].tag2id),\n",
    "    num_sequence_labels=len(dataset['train'].class2id)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "model_fp32_trained = train(train_loader, model_fp32, epochs=EPOCHS, max_grad_norm=MAX_GRAD_NORM, amx=False, dataType='fp32')\n",
    "training_time_fp32 = time.time()\n",
    "\n",
    "# Evaluate accuracy on the testing set in batches\n",
    "accuracy_ner_fp32, accuracy_class_fp32 = evaluate_accuracy(test_loader, model_fp32_trained)\n",
    "testing_time_fp32 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528fab99-3472-4842-960f-4ad82ccf0f9c",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. Â© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103d2c1-8265-4ea2-8782-e979a88be6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TIME METRICS')\n",
    "\n",
    "print(\"=======> FP32 ONLY - Test Accuracy on NER: \", accuracy_ner_fp32)\n",
    "print(\"=======> FP32 ONLY - Test Accuracy on CLS: \", accuracy_class_fp32)\n",
    "print(\"=======> FP32 ONLY - Training Time   mins\", (training_time_fp32 - start_fp32)/60)\n",
    "print(\"=======> FP32 ONLY - Inference Time:  secs\", (testing_time_fp32 - training_time_fp32))\n",
    "print(\"=======> FP32 ONLY - Total Time:  mins\", (testing_time_fp32 - start_fp32)/60)\n",
    "\n",
    "print('-'*100)\n",
    "\n",
    "print(\"=======> BF16 with AMX - Test Accuracy on NER : \", accuracy_ner_bf16_wAMX)\n",
    "print(\"=======> BF16 with AMX - Test Accuracy on CLS : \", accuracy_class_bf16_wAMX)\n",
    "print(\"=======> BF16 with AMX - Training Time:  mins\", (training_time_bf16_wAMX - start_bf16_wAMX)/60)\n",
    "print(\"=======> BF16 with AMX - Inference Time:  secs\", (testing_time_bf16_wAMX - training_time_bf16_wAMX))\n",
    "print(\"=======> BF16 with AMX - Total Time:  mins\", (testing_time_bf16_wAMX - start_bf16_wAMX)/60)\n",
    "\n",
    "fp32_training_time = (training_time_fp32 - start_fp32)\n",
    "bf16_wAMX_training_time = (training_time_bf16_wAMX - start_bf16_wAMX)\n",
    "\n",
    "fp32_inference_time = (testing_time_fp32 - training_time_fp32)\n",
    "bf16_wAMX_inference_time = (testing_time_bf16_wAMX - training_time_bf16_wAMX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ec96b-da99-4d68-83ef-6f99c6ded7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.title(\" Training Time\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Training Time (seconds)\")\n",
    "plt.bar([\"FP32\", \"BF16 with AMX\"], [fp32_training_time, bf16_wAMX_training_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7252990-f40d-4811-a128-ea7e49a3a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf16AMX_Uplift_over_FP32 = (fp32_training_time/bf16_wAMX_training_time)\n",
    "\n",
    "print(\"BF16 with AMX is %.2fX faster than FP32 in Training\" %bf16AMX_Uplift_over_FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e416ae8-9d90-4dab-a195-93f431b097c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\" Inference Time\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Inference Time (seconds)\")\n",
    "plt.bar([\"FP32\", \"BF16 with AMX\"], [fp32_inference_time, bf16_wAMX_inference_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6526df8-a23c-40f1-9f22-ff0a104deb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf16AMX_Uplift_over_FP32 = (fp32_inference_time/bf16_wAMX_inference_time)\n",
    "\n",
    "print(\"BF16 with AMX is %.2fX faster than FP32 in Inference\" %bf16AMX_Uplift_over_FP32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idp",
   "language": "python",
   "name": "idp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
